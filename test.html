<!DOCTYPE html>

<html>
  <body>
    <p id ="txttotest"></p>
    <input type="text" id="myText" value="Enter text here">
    <button onclick="toxicityScan()">Try it</button>
      
    <p id ="txttocensor1">Hi, I'm from idaho.</p>
    <p id ="txttocensor2">you suck</p>
    <p id ="txttocensor3">bitch ass</p>
  </body>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity"></script>
    
    <script>
      
      function toxicityScan() {
      
        // The minimum prediction confidence.
        const threshold = 0.9;

        // Load the model. Users optionally pass in a threshold and an array of
        // labels to include.
        toxicity.load(threshold).then(model => {
          var x = document.getElementById("myText").value;
          //const sentences = ['you suck'];
          const sentences = [`${x}`];
          //console.log(sentences);

          model.classify(sentences).then(predictions => {
            // `predictions` is an array of objects, one for each prediction head,
            // that contains the raw probabilities for each input along with the
            // final prediction in `match` (either `true` or `false`).
            // If neither prediction exceeds the threshold, `match` is `null`.

            console.log(predictions);
            console.log(sentences);
            document.getElementById("txttotest").innerHTML = `Scanned phrase: ${x}`;
            
            var toxicityArray = Object.values(predictions)
            var toxicityArrayBooleans = [ //list each independently as we might choose to remove some later 
                toxicityArray[0].results[0]["match"], //identity_attack
                toxicityArray[1].results[0]["match"], //insult
                toxicityArray[2].results[0]["match"], //obscene
                toxicityArray[3].results[0]["match"], //severe_toxicity
                toxicityArray[4].results[0]["match"], //sexual_explicit
                toxicityArray[5].results[0]["match"], //threat
                toxicityArray[6].results[0]["match"]  // toxicity
                ];
            
            
            console.log(toxicityArrayBooleans);
            
            /*Seems like pure toxicity toxicityArray[6] is the most useful one by far. 
            Perhaps if we cut the others out we can speed it up enough to run it client-side 
            */
              
            if (toxicityArray[6].results[0]["match"]){
                console.log("Toxicity detected");
            }
            
              
            /*
            prints:
            {
              "label": "identity_attack",
              "results": [{
                "probabilities": [0.9659664034843445, 0.03403361141681671],
                "match": false
              }]
            },
            {
              "label": "insult",
              "results": [{
                "probabilities": [0.08124706149101257, 0.9187529683113098],
                "match": true
              }]
            },
            ...
            */
          });
        });
      }
    </script>
    
    <script>
    </script>
    
</html>